{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37272949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import torch\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from facenet_pytorch import InceptionResnetV1, MTCNN\n",
    "from deepface import DeepFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b17b4e",
   "metadata": {},
   "source": [
    "## Carregamento e Transformação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc1fa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_seconds(t):\n",
    "    h, m, s_ms = t.split(\":\")\n",
    "    s, ms = s_ms.split(\",\")\n",
    "    return int(h)*3600 + int(m)*60 + int(s) + int(ms)/1000\n",
    "\n",
    "def to_snake_case(name: str) -> str:\n",
    "    \"\"\"Convert a string (like a column name) to snake_case.\"\"\"\n",
    "    name = name.strip()\n",
    "    name = re.sub(r\"[^\\w\\s]\", \"\", name)\n",
    "    name = re.sub(r\"\\s+\", \"_\", name)\n",
    "    return name.lower()\n",
    "\n",
    "def prepare_df(df: pd.DataFrame):\n",
    "    df[\"start_s\"] = df[\"StartTime\"].apply(to_seconds)\n",
    "    df[\"end_s\"] = df[\"EndTime\"].apply(to_seconds)\n",
    "    df[\"duration_s\"] = df[\"end_s\"] - df[\"start_s\"]\n",
    "\n",
    "    df[df[\"duration_s\"] <= 0].head()\n",
    "\n",
    "    df = df.drop(columns=[\"Sr No.\", \"StartTime\", \"EndTime\", \"Season\", \"Episode\", \"Sentiment\"])\n",
    "\n",
    "    df.columns = [to_snake_case(c) for c in df.columns]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930e2068",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = prepare_df(pd.read_csv(\"./data/train_sent_emo.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235f7e33",
   "metadata": {},
   "source": [
    "## Funções base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b5fd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c6ed9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseExtractor:\n",
    "    def __init__(self, save_dir):\n",
    "        self.save_dir = Path(save_dir)\n",
    "        self.save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def save(self, dialogue_id, utterance_id, vector):\n",
    "        path = self.save_dir / f\"{dialogue_id}_{utterance_id}.npy\"\n",
    "        np.save(path, vector)\n",
    "        return str(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eade09",
   "metadata": {},
   "source": [
    "## Features Faciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb53af6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoExtractor(BaseExtractor):\n",
    "    def __init__(self, model_name, save_dir, cfg):\n",
    "        super().__init__(save_dir)\n",
    "        self.model_name = model_name.lower()\n",
    "        self.cfg = cfg\n",
    "        self.frame_step = cfg.get(\"frame_step\", 5)\n",
    "        self.resize = cfg.get(\"resize\", 160)\n",
    "\n",
    "        if \"facenet\" in self.model_name:\n",
    "            self.detector = MTCNN(keep_all=False, device=\"cpu\")\n",
    "            self.model = InceptionResnetV1(pretrained=\"vggface2\").eval()\n",
    "        elif \"deepface\" in self.model_name or \"vgg-face\" in self.model_name:\n",
    "            self.detector_backend = cfg.get(\"detector_backend\", \"opencv\")\n",
    "            self.deepface = DeepFace\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported video model: {self.model_name}\")\n",
    "\n",
    "    def extract(self, video_path):\n",
    "        \"\"\"Extract averaged face embedding for one utterance clip (.mp4).\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            raise RuntimeError(f\"Could not open {video_path}\")\n",
    "\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS) or 25\n",
    "        frame_step = self.frame_step\n",
    "\n",
    "        frame_id = 0\n",
    "        embeddings = []\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            if frame_id % frame_step != 0:\n",
    "                frame_id += 1\n",
    "                continue\n",
    "\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            if \"facenet\" in self.model_name:\n",
    "                boxes, probs = self.detector.detect(frame_rgb)\n",
    "                if boxes is not None and len(boxes) > 0:\n",
    "                    x1, y1, x2, y2 = boxes[0].astype(int)\n",
    "                    face = frame_rgb[y1:y2, x1:x2]\n",
    "                    face = cv2.resize(face, (self.resize, self.resize))\n",
    "                    face = np.transpose(face, (2, 0, 1)) / 255.0\n",
    "                    face = torch.tensor(face).unsqueeze(0).float()\n",
    "                    with torch.no_grad():\n",
    "                        emb = self.model(face).squeeze().numpy()\n",
    "                    embeddings.append(emb)\n",
    "\n",
    "            elif \"deepface\" in self.model_name or \"vgg-face\" in self.model_name:\n",
    "                try:\n",
    "                    rep = self.deepface.represent(\n",
    "                        img_path=frame_rgb,\n",
    "                        model_name=self.model_name.upper(),\n",
    "                        detector_backend=self.detector_backend,\n",
    "                        enforce_detection=False,\n",
    "                    )\n",
    "                    if len(rep) > 0:\n",
    "                        emb = np.array(rep[0][\"embedding\"])\n",
    "                        embeddings.append(emb)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            frame_id += 1\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        if len(embeddings) == 0:\n",
    "            return np.zeros(128)\n",
    "\n",
    "        return np.mean(np.stack(embeddings), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0e5804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_video_with_progress(df: pd.DataFrame, extractor: VideoExtractor, save_dir: str, video_dir: str):\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Video extracting\", ncols=80):\n",
    "        filename = f\"{row['dialogue_id']}_{row['utterance_id']}.npy\"\n",
    "        out_path = save_dir / filename\n",
    "        \n",
    "        if out_path.exists():\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            vec = extractor.extract(f\"{video_dir}/dia{row['dialogue_id']}_utt{row['utterance_id']}.mp4\")\n",
    "            extractor.save(row[\"dialogue_id\"], row[\"utterance_id\"], vec)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error on {video_dir}: {e}\")\n",
    "            continue\n",
    "        \n",
    "\n",
    "    print(f\"✅ Saved embeddings in {save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f1b74c",
   "metadata": {},
   "source": [
    "### Deepface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54fe40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = load_config(\"configs/faces/deepface.yaml\")\n",
    "extractor = VideoExtractor(cfg[\"model_name\"], cfg[\"save_dir\"], cfg)\n",
    "\n",
    "extract_video_with_progress(train_df, extractor, cfg[\"save_dir\"], \"./data/train/train_splits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5699049c",
   "metadata": {},
   "source": [
    "### Facenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb8f30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = load_config(\"configs/faces/facenet.yaml\")\n",
    "extractor = VideoExtractor(cfg[\"model_name\"], cfg[\"save_dir\"], cfg)\n",
    "\n",
    "extract_video_with_progress(train_df, extractor, cfg[\"save_dir\"], \"./data/train/train_splits\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
