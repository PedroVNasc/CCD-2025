{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37272949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import torch\n",
    "import torchaudio\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModel, Wav2Vec2Processor, Wav2Vec2Model\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b17b4e",
   "metadata": {},
   "source": [
    "## Carregamento e Transformação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc1fa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_seconds(t):\n",
    "    h, m, s_ms = t.split(\":\")\n",
    "    s, ms = s_ms.split(\",\")\n",
    "    return int(h)*3600 + int(m)*60 + int(s) + int(ms)/1000\n",
    "\n",
    "def to_snake_case(name: str) -> str:\n",
    "    \"\"\"Convert a string (like a column name) to snake_case.\"\"\"\n",
    "    name = name.strip()\n",
    "    name = re.sub(r\"[^\\w\\s]\", \"\", name)\n",
    "    name = re.sub(r\"\\s+\", \"_\", name)\n",
    "    return name.lower()\n",
    "\n",
    "def prepare_df(df: pd.DataFrame):\n",
    "    df[\"start_s\"] = df[\"StartTime\"].apply(to_seconds)\n",
    "    df[\"end_s\"] = df[\"EndTime\"].apply(to_seconds)\n",
    "    df[\"duration_s\"] = df[\"end_s\"] - df[\"start_s\"]\n",
    "\n",
    "    df[df[\"duration_s\"] <= 0].head()\n",
    "\n",
    "    df = df.drop(columns=[\"Sr No.\", \"StartTime\", \"EndTime\", \"Season\", \"Episode\", \"Sentiment\"])\n",
    "\n",
    "    df.columns = [to_snake_case(c) for c in df.columns]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930e2068",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = prepare_df(pd.read_csv(\"./data/train_sent_emo.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235f7e33",
   "metadata": {},
   "source": [
    "## Funções base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b5fd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c6ed9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseExtractor:\n",
    "    def __init__(self, save_dir):\n",
    "        self.save_dir = Path(save_dir)\n",
    "        self.save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def save(self, dialogue_id, utterance_id, vector):\n",
    "        path = self.save_dir / f\"{dialogue_id}_{utterance_id}.npy\"\n",
    "        np.save(path, vector)\n",
    "        return str(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50866e2f",
   "metadata": {},
   "source": [
    "## Features de Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84c32a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextExtractor(BaseExtractor):\n",
    "    def __init__(self, model_name, save_dir, device=\"cpu\"):\n",
    "        super().__init__(save_dir)\n",
    "        self.device = device\n",
    "        self.tok = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "    def extract(self, text):\n",
    "        inputs = self.tok(text, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs).last_hidden_state[:, 0, :]  # [CLS]\n",
    "        return outputs.squeeze().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4371b26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_with_progress(df, extractor, save_dir):\n",
    "    \"\"\"\n",
    "    Extract features for all utterances, resuming if interrupted.\n",
    "    \n",
    "    - Skips already existing .npy files\n",
    "    - Displays tqdm progress bar\n",
    "    \"\"\"\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    total = len(df)\n",
    "    processed = 0\n",
    "\n",
    "    if len(list(save_dir.glob(\"*.npy\"))) >= len(df):\n",
    "        print(f\"✅ Already processed {len(df)} utterances. Skipping.\")\n",
    "        return\n",
    "\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=total, desc=\"Extracting\", ncols=80):\n",
    "        filename = f\"{row['dialogue_id']}_{row['utterance_id']}.npy\"\n",
    "        out_path = save_dir / filename\n",
    "\n",
    "        if out_path.exists():\n",
    "            processed += 1\n",
    "            continue\n",
    "\n",
    "        vec = extractor.extract(row[\"utterance\"])\n",
    "        extractor.save(row[\"dialogue_id\"], row[\"utterance_id\"], vec)\n",
    "\n",
    "        processed += 1\n",
    "\n",
    "    print(f\"\\n✅ Completed: {processed}/{total} utterances processed.\")\n",
    "    print(f\"Features saved in: {save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56db30da",
   "metadata": {},
   "source": [
    "### Roberta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a2fb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = load_config(\"configs/text/roberta.yaml\")\n",
    "extractor = TextExtractor(cfg[\"model_name\"], cfg[\"save_dir\"], cfg[\"device\"])\n",
    "\n",
    "extract_text_with_progress(train_df, extractor, cfg[\"save_dir\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0283971",
   "metadata": {},
   "source": [
    "### DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f223a4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = load_config(\"configs/text/distilbert.yaml\")\n",
    "extractor = TextExtractor(cfg[\"model_name\"], cfg[\"save_dir\"], cfg[\"device\"])\n",
    "\n",
    "extract_text_with_progress(train_df, extractor, cfg[\"save_dir\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc10182",
   "metadata": {},
   "source": [
    "### MpNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605db36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = load_config(\"configs/text/mpnet.yaml\")\n",
    "extractor = TextExtractor(cfg[\"model_name\"], cfg[\"save_dir\"], cfg[\"device\"])\n",
    "\n",
    "extract_text_with_progress(train_df, extractor, cfg[\"save_dir\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82788df",
   "metadata": {},
   "source": [
    "## Features de Áudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd53a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioExtractor(BaseExtractor):\n",
    "    def __init__(self, model_name, save_dir, cfg):\n",
    "        super().__init__(save_dir)\n",
    "        self.model_name = model_name.lower()\n",
    "        self.cfg = cfg\n",
    "        self.sample_rate = cfg.get(\"sample_rate\", 16000)\n",
    "\n",
    "        # --- MFCC ---\n",
    "        if self.model_name == \"mfcc\":\n",
    "            self.extractor = torchaudio.transforms.MFCC(\n",
    "                sample_rate=self.sample_rate,\n",
    "                n_mfcc=cfg.get(\"n_mfcc\", 13),\n",
    "                melkwargs={\n",
    "                    \"n_fft\": cfg.get(\"n_fft\", 400),\n",
    "                    \"hop_length\": cfg.get(\"hop_length\", 160),\n",
    "                    \"n_mels\": cfg.get(\"n_mels\", 23),\n",
    "                },\n",
    "            )\n",
    "\n",
    "        # --- MelSpectrogram ---\n",
    "        elif self.model_name == \"melspectrogram\":\n",
    "            self.extractor = torchaudio.transforms.MelSpectrogram(\n",
    "                sample_rate=self.sample_rate,\n",
    "                n_fft=cfg.get(\"n_fft\", 400),\n",
    "                hop_length=cfg.get(\"hop_length\", 160),\n",
    "                n_mels=cfg.get(\"n_mels\", 64),\n",
    "            )\n",
    "\n",
    "        # --- Wav2Vec2 / Wav2Vec2-like ---\n",
    "        elif \"wav2vec\" in self.model_name:\n",
    "            self.processor = Wav2Vec2Processor.from_pretrained(\n",
    "                self.model_name, use_safetensors=True\n",
    "            )\n",
    "            self.model = Wav2Vec2Model.from_pretrained(self.model_name)\n",
    "            self.model.eval()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {self.model_name}\")\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "\n",
    "    def extract(self, audio_path: str):\n",
    "        # Load the audio directly from .mp4 (requires ffmpeg in system or Conda)\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "\n",
    "        # Convert to mono if stereo\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "        # Resample if necessary\n",
    "        if sr != self.sample_rate:\n",
    "            waveform = torchaudio.functional.resample(\n",
    "                waveform, sr, self.sample_rate\n",
    "            )\n",
    "\n",
    "        # Ensure consistent max duration (avoid huge clips)\n",
    "        max_len_seconds = self.cfg.get(\"max_len_seconds\", 30)\n",
    "        max_len_samples = int(max_len_seconds * self.sample_rate)\n",
    "        if waveform.shape[1] > max_len_samples:\n",
    "            waveform = waveform[:, :max_len_samples]\n",
    "\n",
    "        # -------- MFCC or MelSpectrogram --------\n",
    "        if self.model_name in [\"mfcc\", \"melspectrogram\"]:\n",
    "            feat = self.extractor(waveform)\n",
    "            vec = feat.mean(dim=-1).squeeze().numpy()\n",
    "            return vec\n",
    "\n",
    "        elif \"wav2vec\" in self.model_name:\n",
    "            with torch.no_grad():\n",
    "                inputs = self.processor(\n",
    "                    waveform.squeeze(),\n",
    "                    sampling_rate=self.sample_rate,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                )\n",
    "                outputs = self.model(**inputs).last_hidden_state\n",
    "                vec = outputs.mean(dim=1).squeeze().cpu().numpy()\n",
    "            return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a163ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_with_progress(df: pd.DataFrame, extractor: AudioExtractor, save_dir: str, audio_dir: str):\n",
    "    \"\"\"\n",
    "    Extract audio features for all utterances, continuing from previous progress.\n",
    "    \"\"\"\n",
    "\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    total = len(df)\n",
    "    processed = 0\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=total, desc=\"Audio extracting\", ncols=80):\n",
    "        filename = f\"{row['dialogue_id']}_{row['utterance_id']}.npy\"\n",
    "        out_path = save_dir / filename\n",
    "        \n",
    "        if out_path.exists():\n",
    "            processed += 1\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            vec = extractor.extract(f\"{audio_dir}/dia{row['dialogue_id']}_utt{row['utterance_id']}.mp4\")\n",
    "            extractor.save(row[\"dialogue_id\"], row[\"utterance_id\"], vec)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error on {audio_dir}/dia{row['dialogue_id']}_utt{row['utterance_id']}.mp4: {e}\")\n",
    "            continue\n",
    "        \n",
    "\n",
    "        processed += 1\n",
    "\n",
    "    print(f\"\\n✅ Completed {processed}/{total} utterances.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46203d31",
   "metadata": {},
   "source": [
    "### MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a9eb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = load_config(\"configs/audio/mfcc.yaml\")\n",
    "extractor = AudioExtractor(cfg[\"model_name\"], cfg[\"save_dir\"], cfg)\n",
    "\n",
    "extract_audio_with_progress(train_df, extractor, cfg[\"save_dir\"], \"./data/train/train_splits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d90c478",
   "metadata": {},
   "source": [
    "### MelSpectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69acb3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = load_config(\"configs/audio/melspec.yaml\")\n",
    "extractor = AudioExtractor(cfg[\"model_name\"], cfg[\"save_dir\"], cfg)\n",
    "\n",
    "extract_audio_with_progress(train_df, extractor, cfg[\"save_dir\"], \"./data/train/train_splits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ab23e5",
   "metadata": {},
   "source": [
    "### Wav2Vec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91224699",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = load_config(\"configs/audio/wav2vec2.yaml\")\n",
    "extractor = AudioExtractor(cfg[\"model_name\"], cfg[\"save_dir\"], cfg)\n",
    "\n",
    "extract_audio_with_progress(train_df, extractor, cfg[\"save_dir\"], \"./data/train/train_splits\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
