{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37272949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "import torch\n",
    "import torchaudio\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers.models.wav2vec2 import Wav2Vec2Processor, Wav2Vec2Model\n",
    "from tqdm import tqdm\n",
    "from facenet_pytorch import InceptionResnetV1, MTCNN\n",
    "from deepface import DeepFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2f452d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIRS = {\n",
    "    \"train\": \"./data/train/train_splits\",\n",
    "    \"val\": \"./data/dev/dev_splits_complete\",\n",
    "    \"test\": \"./data/test/output_repeated_splits_test\"\n",
    "}\n",
    "\n",
    "OUTPUT_BASE = \"./output\"\n",
    "\n",
    "os.makedirs(OUTPUT_BASE, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78f4b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_speakers = [\"Chandler\", \"Monica\", \"Ross\", \"Rachel\", \"Phoebe\", \"Joey\"]\n",
    "emotions = [\"neutral\", \"joy\", \"surprise\", \"anger\", \"sadness\", \"disgust\", \"fear\"]\n",
    "\n",
    "viridis_colors = plt.cm.viridis(np.linspace(0, 1, len(emotions)))\n",
    "emotion_colors = dict(zip(emotions, viridis_colors))\n",
    "\n",
    "emotion_colors[\"neutral\"] = \"#808080\"\n",
    "emotion_colors[\"anger\"] = \"#DC143C\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b17b4e",
   "metadata": {},
   "source": [
    "## Carregamento e Transformação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc1fa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_seconds(t):\n",
    "    h, m, s_ms = t.split(\":\")\n",
    "    s, ms = s_ms.split(\",\")\n",
    "    return int(h)*3600 + int(m)*60 + int(s) + int(ms)/1000\n",
    "\n",
    "def to_snake_case(name: str) -> str:\n",
    "    \"\"\"Convert a string (like a column name) to snake_case.\"\"\"\n",
    "    name = name.strip()\n",
    "    name = re.sub(r\"[^\\w\\s]\", \"\", name)\n",
    "    name = re.sub(r\"\\s+\", \"_\", name)\n",
    "    return name.lower()\n",
    "\n",
    "def prepare_df(df: pd.DataFrame):\n",
    "    df[\"start_s\"] = df[\"StartTime\"].apply(to_seconds)\n",
    "    df[\"end_s\"] = df[\"EndTime\"].apply(to_seconds)\n",
    "    df[\"duration_s\"] = df[\"end_s\"] - df[\"start_s\"]\n",
    "\n",
    "    df[df[\"duration_s\"] <= 0].head()\n",
    "\n",
    "    df = df.drop(columns=[\"Sr No.\", \"StartTime\", \"EndTime\", \"Season\", \"Episode\", \"Sentiment\"])\n",
    "\n",
    "    df.columns = [to_snake_case(c) for c in df.columns]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930e2068",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = prepare_df(pd.read_csv(\"./data/train_sent_emo.csv\"))\n",
    "test_df = prepare_df(pd.read_csv(\"./data/test_sent_emo.csv\"))\n",
    "val_df = prepare_df(pd.read_csv(\"./data/dev_sent_emo.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235f7e33",
   "metadata": {},
   "source": [
    "## Funções base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b5fd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c6ed9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseExtractor:\n",
    "    def __init__(self, save_dir):\n",
    "        self.save_dir = Path(save_dir)\n",
    "        self.save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def save(self, dialogue_id, utterance_id, vector):\n",
    "        path = self.save_dir / f\"{dialogue_id}_{utterance_id}.npy\"\n",
    "        np.save(path, vector)\n",
    "        return str(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50866e2f",
   "metadata": {},
   "source": [
    "## Features de Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84c32a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextExtractor(BaseExtractor):\n",
    "    def __init__(self, model_name, save_dir, device=\"cpu\"):\n",
    "        super().__init__(save_dir)\n",
    "        self.device = device\n",
    "        self.tok = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "    def extract(self, text):\n",
    "        inputs = self.tok(text, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs).last_hidden_state[:, 0, :]  # [CLS]\n",
    "        return outputs.squeeze().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4371b26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_with_progress(df, extractor, save_dir):\n",
    "    \"\"\"\n",
    "    Extract features for all utterances, resuming if interrupted.\n",
    "    \n",
    "    - Skips already existing .npy files\n",
    "    - Displays tqdm progress bar\n",
    "    \"\"\"\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    total = len(df)\n",
    "    processed = 0\n",
    "\n",
    "    if len(list(save_dir.glob(\"*.npy\"))) >= len(df):\n",
    "        print(f\"✅ Already processed {len(df)} utterances. Skipping.\")\n",
    "        return\n",
    "\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=total, desc=\"Extracting\", ncols=80):\n",
    "        filename = f\"{row['dialogue_id']}_{row['utterance_id']}.npy\"\n",
    "        out_path = save_dir / filename\n",
    "\n",
    "        if out_path.exists():\n",
    "            processed += 1\n",
    "            continue\n",
    "\n",
    "        vec = extractor.extract(row[\"utterance\"])\n",
    "        extractor.save(row[\"dialogue_id\"], row[\"utterance_id\"], vec)\n",
    "\n",
    "        processed += 1\n",
    "\n",
    "    print(f\"\\n✅ Completed: {processed}/{total} utterances processed.\")\n",
    "    print(f\"Features saved in: {save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56db30da",
   "metadata": {},
   "source": [
    "### Roberta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a2fb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = load_config(\"configs/text/roberta.yaml\")\n",
    "extractor = TextExtractor(cfg[\"model_name\"], cfg[\"save_dir\"], cfg[\"device\"])\n",
    "\n",
    "extract_text_with_progress(train_df, extractor, cfg[\"save_dir\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0283971",
   "metadata": {},
   "source": [
    "### DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f223a4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = load_config(\"configs/text/distilbert.yaml\")\n",
    "extractor = TextExtractor(cfg[\"model_name\"], cfg[\"save_dir\"], cfg[\"device\"])\n",
    "\n",
    "extract_text_with_progress(train_df, extractor, cfg[\"save_dir\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc10182",
   "metadata": {},
   "source": [
    "### MpNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605db36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = load_config(\"configs/text/mpnet.yaml\")\n",
    "extractor = TextExtractor(cfg[\"model_name\"], cfg[\"save_dir\"], cfg[\"device\"])\n",
    "\n",
    "extract_text_with_progress(train_df, extractor, cfg[\"save_dir\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82788df",
   "metadata": {},
   "source": [
    "## Features de Áudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd53a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "\n",
    "class AudioExtractor(BaseExtractor):\n",
    "    def __init__(self, model_name, save_dir, cfg):\n",
    "        super().__init__(save_dir)\n",
    "        self.model_name = model_name.lower()\n",
    "        self.cfg = cfg\n",
    "        self.sample_rate = cfg.get(\"sample_rate\", 16000)\n",
    "\n",
    "        # --- MFCC ---\n",
    "        if self.model_name == \"mfcc\":\n",
    "            self.extractor = torchaudio.transforms.MFCC(\n",
    "                sample_rate=self.sample_rate,\n",
    "                n_mfcc=cfg.get(\"n_mfcc\", 13),\n",
    "                melkwargs={\n",
    "                    \"n_fft\": cfg.get(\"n_fft\", 400),\n",
    "                    \"hop_length\": cfg.get(\"hop_length\", 160),\n",
    "                    \"n_mels\": cfg.get(\"n_mels\", 23),\n",
    "                },\n",
    "            )\n",
    "\n",
    "        # --- MelSpectrogram ---\n",
    "        elif self.model_name == \"melspectrogram\":\n",
    "            self.extractor = torchaudio.transforms.MelSpectrogram(\n",
    "                sample_rate=self.sample_rate,\n",
    "                n_fft=cfg.get(\"n_fft\", 400),\n",
    "                hop_length=cfg.get(\"hop_length\", 160),\n",
    "                n_mels=cfg.get(\"n_mels\", 64),\n",
    "            )\n",
    "\n",
    "        # --- Wav2Vec2 / Wav2Vec2-like ---\n",
    "        elif \"wav2vec\" in self.model_name:\n",
    "            self.processor = Wav2Vec2Processor.from_pretrained(\n",
    "                self.model_name, use_safetensors=True\n",
    "            )\n",
    "            self.model = Wav2Vec2Model.from_pretrained(self.model_name)\n",
    "            self.model.eval()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {self.model_name}\")\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "\n",
    "    def extract(self, audio_path: str):\n",
    "        # Load the audio directly from .mp4 (requires ffmpeg in system or Conda)\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "\n",
    "        # Convert to mono if stereo\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "        # Resample if necessary\n",
    "        if sr != self.sample_rate:\n",
    "            waveform = torchaudio.functional.resample(\n",
    "                waveform, sr, self.sample_rate\n",
    "            )\n",
    "\n",
    "        # Ensure consistent max duration (avoid huge clips)\n",
    "        max_len_seconds = self.cfg.get(\"max_len_seconds\", 30)\n",
    "        max_len_samples = int(max_len_seconds * self.sample_rate)\n",
    "        if waveform.shape[1] > max_len_samples:\n",
    "            waveform = waveform[:, :max_len_samples]\n",
    "\n",
    "        # -------- MFCC or MelSpectrogram --------\n",
    "        if self.model_name in [\"mfcc\", \"melspectrogram\"]:\n",
    "            feat = self.extractor(waveform)\n",
    "            vec = feat.mean(dim=-1).squeeze().numpy()\n",
    "            return vec\n",
    "\n",
    "        elif \"wav2vec\" in self.model_name:\n",
    "            with torch.no_grad():\n",
    "                inputs = self.processor(\n",
    "                    waveform.squeeze(),\n",
    "                    sampling_rate=self.sample_rate,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                )\n",
    "                outputs = self.model(**inputs).last_hidden_state\n",
    "                vec = outputs.mean(dim=1).squeeze().cpu().numpy()\n",
    "            return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1a163ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_with_progress(df: pd.DataFrame, extractor: AudioExtractor, save_dir: str, audio_dir: str):\n",
    "    \"\"\"\n",
    "    Extract audio features for all utterances, continuing from previous progress.\n",
    "    \"\"\"\n",
    "\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    total = len(df)\n",
    "    processed = 0\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=total, desc=\"Audio extracting\", ncols=80):\n",
    "        filename = f\"{row['dialogue_id']}_{row['utterance_id']}.npy\"\n",
    "        out_path = save_dir / filename\n",
    "        \n",
    "        if out_path.exists():\n",
    "            processed += 1\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            vec = extractor.extract(f\"{audio_dir}/dia{row['dialogue_id']}_utt{row['utterance_id']}.mp4\")\n",
    "            extractor.save(row[\"dialogue_id\"], row[\"utterance_id\"], vec)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error on {audio_dir}/dia{row['dialogue_id']}_utt{row['utterance_id']}.mp4: {e}\")\n",
    "            continue\n",
    "        \n",
    "\n",
    "        processed += 1\n",
    "\n",
    "    print(f\"\\n✅ Completed {processed}/{total} utterances.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46203d31",
   "metadata": {},
   "source": [
    "### MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1a9eb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Audio extracting:  29%|█████▍             | 2890/9989 [00:00<00:01, 6063.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error on ./data/train/train_splits/dia125_utt3.mp4: Failed to create AudioDecoder for ./data/train/train_splits/dia125_utt3.mp4: Could not open input file: ./data/train/train_splits/dia125_utt3.mp4 Invalid data found when processing input\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Audio extracting: 100%|█████████████████████| 9989/9989 [05:25<00:00, 30.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Completed 9988/9989 utterances.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cfg = load_config(\"configs/audio/mfcc.yaml\")\n",
    "extractor = AudioExtractor(cfg[\"model_name\"], cfg[\"save_dir\"], cfg)\n",
    "\n",
    "extract_audio_with_progress(train_df, extractor, cfg[\"save_dir\"], \"./data/train/train_splits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d90c478",
   "metadata": {},
   "source": [
    "### MelSpectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69acb3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Audio extracting:  12%|██▍                  | 1169/9989 [00:47<08:42, 16.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error on ./data/train/train_splits/dia125_utt3.mp4: Failed to create AudioDecoder for ./data/train/train_splits/dia125_utt3.mp4: Could not open input file: ./data/train/train_splits/dia125_utt3.mp4 Invalid data found when processing input\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Audio extracting: 100%|█████████████████████| 9989/9989 [08:16<00:00, 20.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Completed 9988/9989 utterances.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cfg = load_config(\"configs/audio/melspec.yaml\")\n",
    "extractor = AudioExtractor(cfg[\"model_name\"], cfg[\"save_dir\"], cfg)\n",
    "\n",
    "extract_audio_with_progress(train_df, extractor, cfg[\"save_dir\"], \"./data/train/train_splits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ab23e5",
   "metadata": {},
   "source": [
    "### Wav2Vec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91224699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Audio extracting:  12%|██▏                | 1166/9989 [32:16<4:08:49,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error on ./data/train/train_splits/dia125_utt3.mp4: Failed to create AudioDecoder for ./data/train/train_splits/dia125_utt3.mp4: Could not open input file: ./data/train/train_splits/dia125_utt3.mp4 Invalid data found when processing input\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Audio extracting:  83%|██████████████   | 8251/9989 [4:09:13<1:25:17,  2.94s/it]"
     ]
    }
   ],
   "source": [
    "cfg = load_config(\"configs/audio/wav2vec2.yaml\")\n",
    "extractor = AudioExtractor(cfg[\"model_name\"], cfg[\"save_dir\"], cfg)\n",
    "\n",
    "extract_audio_with_progress(train_df, extractor, cfg[\"save_dir\"], \"./data/train/train_splits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eade09",
   "metadata": {},
   "source": [
    "## Features Faciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb53af6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoExtractor(BaseExtractor):\n",
    "    def __init__(self, model_name, save_dir, cfg):\n",
    "        super().__init__(save_dir)\n",
    "        self.model_name = model_name.lower()\n",
    "        self.cfg = cfg\n",
    "        self.frame_step = cfg.get(\"frame_step\", 5)\n",
    "        self.resize = cfg.get(\"resize\", 160)\n",
    "\n",
    "        if \"facenet\" in self.model_name:\n",
    "            self.detector = MTCNN(keep_all=False, device=\"cpu\")\n",
    "            self.model = InceptionResnetV1(pretrained=\"vggface2\").eval()\n",
    "        elif \"deepface\" in self.model_name or \"vgg-face\" in self.model_name:\n",
    "            self.detector_backend = cfg.get(\"detector_backend\", \"opencv\")\n",
    "            self.deepface = DeepFace\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported video model: {self.model_name}\")\n",
    "\n",
    "    def extract(self, video_path):\n",
    "        \"\"\"Extract averaged face embedding for one utterance clip (.mp4).\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            raise RuntimeError(f\"Could not open {video_path}\")\n",
    "\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS) or 25\n",
    "        frame_step = self.frame_step\n",
    "\n",
    "        frame_id = 0\n",
    "        embeddings = []\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            if frame_id % frame_step != 0:\n",
    "                frame_id += 1\n",
    "                continue\n",
    "\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            if \"facenet\" in self.model_name:\n",
    "                boxes, probs = self.detector.detect(frame_rgb)\n",
    "                if boxes is not None and len(boxes) > 0:\n",
    "                    x1, y1, x2, y2 = boxes[0].astype(int)\n",
    "                    face = frame_rgb[y1:y2, x1:x2]\n",
    "                    face = cv2.resize(face, (self.resize, self.resize))\n",
    "                    face = np.transpose(face, (2, 0, 1)) / 255.0\n",
    "                    face = torch.tensor(face).unsqueeze(0).float()\n",
    "                    with torch.no_grad():\n",
    "                        emb = self.model(face).squeeze().numpy()\n",
    "                    embeddings.append(emb)\n",
    "\n",
    "            elif \"deepface\" in self.model_name or \"vgg-face\" in self.model_name:\n",
    "                try:\n",
    "                    rep = self.deepface.represent(\n",
    "                        img_path=frame_rgb,\n",
    "                        model_name=self.model_name.upper(),\n",
    "                        detector_backend=self.detector_backend,\n",
    "                        enforce_detection=False,\n",
    "                    )\n",
    "                    if len(rep) > 0:\n",
    "                        emb = np.array(rep[0][\"embedding\"])\n",
    "                        embeddings.append(emb)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            frame_id += 1\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        if len(embeddings) == 0:\n",
    "            return np.zeros(128)\n",
    "\n",
    "        return np.mean(np.stack(embeddings), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0e5804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_video_with_progress(df: pd.DataFrame, extractor: VideoExtractor, save_dir: str, video_dir: str):\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Video extracting\", ncols=80):\n",
    "        filename = f\"{row['dialogue_id']}_{row['utterance_id']}.npy\"\n",
    "        out_path = save_dir / filename\n",
    "        \n",
    "        if out_path.exists():\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            vec = extractor.extract(f\"{video_dir}/dia{row['dialogue_id']}_utt{row['utterance_id']}.mp4\")\n",
    "            extractor.save(row[\"dialogue_id\"], row[\"utterance_id\"], vec)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error on {video_dir}: {e}\")\n",
    "            continue\n",
    "        \n",
    "\n",
    "    print(f\"✅ Saved embeddings in {save_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
